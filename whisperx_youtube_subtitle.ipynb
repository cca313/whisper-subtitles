{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RQYBkyciDzLu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c596f0b0-65b5-41ad-b1b0-b877561cb1cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 11 09:18:08 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "#define CUDNN_MAJOR 9\n",
            "#define CUDNN_MINOR 2\n",
            "#define CUDNN_PATCHLEVEL 1\n",
            "--\n",
            "#define CUDNN_VERSION (CUDNN_MAJOR * 10000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n",
            "\n",
            "/* cannot use constexpr here since this is a C-only file */\n"
          ]
        }
      ],
      "source": [
        "#@title **通用参数/Required settings:**\n",
        "!nvidia-smi\n",
        "!nvcc -V\n",
        "!cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR -A 2\n",
        "\n",
        "# @markdown **【IMPORTANT】:**<font size=\"2\">Select uploaded file type.\n",
        "# @markdown **</br>【重要】:** 选择上传的文件类型(视频-video/音频-audio）</font>\n",
        "\n",
        "# encoding:utf-8\n",
        "# file_type = \"audio\"  # @param [\"audio\",\"video\"]\n",
        "\n",
        "# @markdown #### **Youtube video**\n",
        "yt_url = \"https://www.youtube.com/watch?v=DJjZzzPANBY\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown #### **Initial prompt**\n",
        "# @markdown Prompts can be very helpful for correcting specific words or acronyms that the model often misrecognizes in the audio.\n",
        "prompt = \"youtube video:Jordan Fisher is the co-founder & CEO of Standard AI and now leads an AI alignment research team at Anthropic. In his talk at AI Startup School on June 17th, 2025, he frames the future of startups through questions rather than answers—asking how founders should navigate a world where AGI may be just a few years away.\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown #### Model\n",
        "model_size = \"large-v3\"  # @param [\"base\", \"base.en\", \"small\", \"small.en\",\"medium\", \"medium.en\", \"large-v1\",\"large-v2\",\"large-v3\"]\n",
        "\n",
        "# @markdown #### Language\n",
        "language = \"auto\" # @param [\"auto\", \"en\", \"zh\", \"de\", \"es\", \"ru\", \"ko\", \"fr\", \"ja\", \"pt\", \"tr\", \"pl\", \"ca\", \"nl\", \"ar\", \"sv\", \"it\", \"id\", \"hi\", \"fi\", \"vi\", \"he\", \"uk\", \"el\", \"ms\", \"cs\", \"ro\", \"da\", \"hu\", \"ta\", \"no\", \"th\", \"ur\", \"hr\", \"bg\", \"lt\", \"la\", \"mi\", \"ml\", \"cy\", \"sk\", \"te\", \"fa\", \"lv\", \"bn\", \"sr\", \"az\", \"sl\", \"kn\", \"et\", \"mk\", \"br\", \"eu\", \"is\", \"hy\", \"ne\", \"mn\", \"bs\", \"kk\", \"sq\", \"sw\", \"gl\", \"mr\", \"pa\", \"si\", \"km\", \"sn\", \"yo\", \"so\", \"af\", \"oc\", \"ka\", \"be\", \"tg\", \"sd\", \"gu\", \"am\", \"yi\", \"lo\", \"uz\", \"fo\", \"ht\", \"ps\", \"tk\", \"nn\", \"mt\", \"sa\", \"lb\", \"my\", \"bo\", \"tl\", \"mg\", \"as\", \"tt\", \"haw\", \"ln\", \"ha\", \"ba\", \"jw\", \"su\"]\n",
        "\n",
        "# @markdown #### Filename Type\n",
        "# @markdown Use YouTube title as file name by default\n",
        "filename_type = \"id\"  # @param [\"title\", \"id\"]\n",
        "\n",
        "# @markdown #### Assign speaker labels\n",
        "# @markdown Recognize speakers\n",
        "assign_speaker_lable = False # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown #### Align whisper output\n",
        "align_whisper_output = True # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WXZPlF99D9jL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ad2e23-db2d-48a4-eecb-158d59b5366d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=DJjZzzPANBY\n",
            "[youtube] DJjZzzPANBY: Downloading webpage\n",
            "[youtube] DJjZzzPANBY: Downloading tv client config\n",
            "[youtube] DJjZzzPANBY: Downloading tv player API JSON\n",
            "[youtube] DJjZzzPANBY: Downloading web safari player API JSON\n",
            "[youtube] DJjZzzPANBY: Downloading player 0004de42-main\n",
            "[youtube] DJjZzzPANBY: Downloading m3u8 information\n",
            "[info] DJjZzzPANBY: Downloading 1 format(s): 140-11\n",
            "[download] Sleeping 2.00 seconds as required by the site...\n",
            "[download] Destination: DJjZzzPANBY.m4a\n",
            "[download] 100% of   37.59MiB in 00:00:00 at 50.00MiB/s  \n",
            "[FixupM4a] Correcting container of \"DJjZzzPANBY.m4a\"\n",
            "[ExtractAudio] Destination: DJjZzzPANBY.wav\n",
            "Deleting original file DJjZzzPANBY.m4a (pass -k to keep)\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=DJjZzzPANBY\n",
            "[youtube] DJjZzzPANBY: Downloading webpage\n",
            "[youtube] DJjZzzPANBY: Downloading tv client config\n",
            "[youtube] DJjZzzPANBY: Downloading tv player API JSON\n",
            "[youtube] DJjZzzPANBY: Downloading web safari player API JSON\n",
            "[youtube] DJjZzzPANBY: Downloading m3u8 information\n",
            "视频文件已保存\n",
            "DJjZzzPANBY.wav\n"
          ]
        }
      ],
      "source": [
        "#@title **运行Whisper/Run Whisper**\n",
        "#@markdown 完成后srt文件将自动下载到本地/srt file will be auto downloaded after finish.\n",
        "\n",
        "! pip install yt_dlp\n",
        "\n",
        "print('开始下载视频')\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "import os\n",
        "import subprocess\n",
        "import yt_dlp\n",
        "import torch\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import requests\n",
        "import sys\n",
        "import gc\n",
        "import re\n",
        "\n",
        "# assert file_name != \"\"\n",
        "# assert language != \"\"\n",
        "tic = time.time()\n",
        "\n",
        "file_name = None\n",
        "\n",
        "outtmpl = '%(title)s.%(ext)s'\n",
        "if filename_type == \"id\":\n",
        "    outtmpl = '%(id)s.%(ext)s'\n",
        "\n",
        "ydl_opts = {\n",
        "    'format': 'm4a/bestaudio/best',\n",
        "    'outtmpl': outtmpl,\n",
        "    # ℹ️ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
        "    'postprocessors': [{  # Extract audio using ffmpeg\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'wav',\n",
        "    }]\n",
        "}\n",
        "\n",
        "\n",
        "title = \"no title\"\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    error_code = ydl.download(yt_url)\n",
        "    info = ydl.extract_info(yt_url, download=False)\n",
        "    title = info['title']\n",
        "\n",
        "    info_with_audio_extension = dict(info)\n",
        "    info_with_audio_extension['ext'] = 'wav'\n",
        "    # Return filename with the correct extension\n",
        "    file_name = ydl.prepare_filename(info_with_audio_extension)\n",
        "\n",
        "file_name = file_name.replace(\".m4a\", \".wav\")\n",
        "print('视频文件已保存')\n",
        "print(file_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kQBMX-uPE5XD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa83ff10-2389-4f84-e556-f3ffcda9628e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 file:/var/cudnn-local-repo-ubuntu2204-9.1.0  InRelease [1,572 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\u001b[0m\r                                                                               \rGet:1 file:/var/cudnn-local-repo-ubuntu2204-9.1.0  InRelease [1,572 B]\n",
            "\u001b[33m\r0% [1 InRelease 0 B/1,572 B 0%] [Connecting to archive.ubuntu.com] [Connecting \u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\u001b[0m\r                                                                               \rHit:2 https://cli.github.com/packages stable InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Waiting for headers] [C\u001b[0m\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Waiting for headers] [C\u001b[0m\r                                                                               \rHit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "LD_LIBRARY_PATH\n",
            "/usr/lib64-nvidia\n",
            "libcudnn9:\n",
            "  Installed: (none)\n",
            "  Candidate: (none)\n",
            "  Version table:\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package libcudnn9-cuda-12 is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "\n",
            "Package libcudnn9-dev-cuda-12 is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  libcudnn9-headers-cuda-13 libcudnn9-headers-cuda-12\n",
            "  libcudnn9-headers-cuda-11\n",
            "\n",
            "E: Version '9.1.0-1' for 'libcudnn9-cuda-12' was not found\n",
            "E: Version '9.1.0-1' for 'libcudnn9-dev-cuda-12' was not found\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "!sudo apt install nvidia-cudnn"
        "# !apt-get remove --purge libcudnn9\n",
        "!sudo apt update\n",
        "# !sudo apt upgrade\n",
        "\n",
        "# !sudo apt-get install cudnn9-cuda-12=9.1.0.70-1\n",
        "# !sudo apt install libcudnn9 libcudnn9-dev -y\n",
        "# !pip install ctranslate2==4.4.0\n",
        "# !pip install pandas==2.2.2\n",
        "# !apt-get install -y --allow-change-held-packages libcudnn9-cuda-12=9.1.0 libcudnn9-dev-cuda-12=9.1.0\n",
        "!echo LD_LIBRARY_PATH\n",
        "print(os.environ['LD_LIBRARY_PATH'])\n",
        "# !echo cuda_lib_path\n",
        "# print(os.environ['cuda_lib_path'])\n",
        "# !ldconfig\n",
        "# !pip install ctranslate2\n",
        "# !cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR -A 2\n",
        "# !pip install git+https://github.com/m-bain/whisperx\n",
        "# !pip install git+https://github.com/m-bain/whisperx\n",
        "# 卸载旧版本的cuDNN（如果需要）\n",
        "# !apt-get remove --purge libcudnn8\n",
        "\n",
        "# 示例：安装cuDNN 9（请根据实际需要的版本进行调整）\n",
        "# 您需要从NVIDIA官网找到对应版本的下载链接\n",
        "# 注意：这通常需要注册一个NVIDIA开发者账号\n",
        "# 以下是一个示例性的安装过程，具体下载链接和文件名需要您去NVIDIA官网查找\n",
        "\n",
        "# 示例下载和安装过程（链接和文件名仅为示例）\n",
        "# !wget https://developer.download.nvidia.com/compute/cudnn/9.1.0/local_installers/cudnn-local-repo-ubuntu2204-9.1.0_1.0-1_amd64.deb\n",
        "# !dpkg -i cudnn-local-repo-ubuntu2204-9.1.0_1.0-1_amd64.deb\n",
        "# !cp /var/cudnn-local-repo-ubuntu2204-9.1.0/cudnn-local-*-keyring.gpg /usr/share/keyrings/\n",
        "!sudo apt policy libcudnn9\n",
        "!sudo apt-get install libcudnn9-cuda-12=9.1.0-1 libcudnn9-dev-cuda-12=9.1.0-1\n",
        "\n",
        "# 如果您知道具体的deb包名，也可以直接安装\n",
        "# !apt-get install -y --allow-change-held-packages libcudnn9=9.1.0.70-1+cuda12.4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"--- PyTorch GPU/cuDNN Verification ---\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✅ Success: PyTorch can access CUDA.\")\n",
        "    print(f\"   - CUDA Version (from PyTorch): {torch.version.cuda}\")\n",
        "    if torch.backends.cudnn.is_available():\n",
        "        print(f\"✅ Success: cuDNN is available.\")\n",
        "        print(f\"   - cuDNN Version (from PyTorch): {torch.backends.cudnn.version()}\")\n",
        "        try:\n",
        "             # 将一个需要 cuDNN 的卷积层移动到GPU上\n",
        "            _ = torch.nn.Conv2d(1, 32, 3).to('cuda')\n",
        "            print(\"✅ cuDNN check passed: A convolutional layer was successfully moved to the GPU.\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failure: An error occurred during cuDNN check: {e}\")\n",
        "    else:\n",
        "        print(\"❌ Failure: cuDNN is not available to PyTorch.\")\n",
        "else:\n",
        "    print(\"❌ Failure: PyTorch cannot access CUDA. Please check installation and runtime type.\")"
      ],
      "metadata": {
        "id": "iWalh0oTlWPb",
        "outputId": "78de01b3-a449-41b0-9983-ea4febbfdea8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- PyTorch GPU/cuDNN Verification ---\n",
            "✅ Success: PyTorch can access CUDA.\n",
            "   - CUDA Version (from PyTorch): 12.6\n",
            "✅ Success: cuDNN is available.\n",
            "   - cuDNN Version (from PyTorch): 91002\n",
            "✅ cuDNN check passed: A convolutional layer was successfully moved to the GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m1rMKyCdEvj9",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0d3b8a7-7f2b-4869-8ef0-ea155c1545a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:3: SyntaxWarning: invalid escape sequence '\\:'\n",
            "<>:3: SyntaxWarning: invalid escape sequence '\\:'\n",
            "/tmp/ipython-input-3442371116.py:3: SyntaxWarning: invalid escape sequence '\\:'\n",
            "  input = file_name.replace(\":\", \"\\:\").replace(\"'\", \"'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whisperx './DJjZzzPANBY.wav' --model large-v3 --output_dir . --initial_prompt \"youtube video:Jordan Fisher is the co-founder & CEO of Standard AI and now leads an AI alignment research team at Anthropic. In his talk at AI Startup School on June 17th, 2025, he frames the future of startups through questions rather than answers—asking how founders should navigate a world where AGI may be just a few years away.\" --align_model WAV2VEC2_ASR_LARGE_LV60K_960H \n",
            "2025-10-11 09:23:34.582301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760174614.603696    1849 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760174614.610285    1849 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760174614.626541    1849 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760174614.626568    1849 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760174614.626573    1849 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760174614.626578    1849 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-11 09:23:34.631781: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/audio/core/io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  torchaudio.list_audio_backends()\n",
            "/usr/local/lib/python3.12/dist-packages/speechbrain/utils/torch_audio_backend.py:57: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  available_backends = torchaudio.list_audio_backends()\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n",
            "config.json: 2.39kB [00:00, 11.9MB/s]\n",
            "vocabulary.json: 0.00B [00:00, ?B/s]\n",
            "preprocessor_config.json: 100% 340/340 [00:00<00:00, 2.02MB/s]\n",
            "\n",
            "vocabulary.json: 1.07MB [00:00, 52.0MB/s]\n",
            "tokenizer.json: 2.48MB [00:00, 126MB/s]\n",
            "model.bin: 100% 3.09G/3.09G [00:53<00:00, 58.2MB/s]\n",
            "2025-10-11 09:24:51 - whisperx.asr - INFO - No language specified, language will be detected for each audio file (increases inference time)\n",
            "2025-10-11 09:24:51 - whisperx.vads.pyannote - INFO - Performing voice activity detection using Pyannote...\n",
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../usr/local/lib/python3.12/dist-packages/whisperx/assets/pytorch_model.bin`\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.8.0+cu126. Bad things might happen unless you revert torch to 1.x.\n",
            "2025-10-11 09:24:58 - whisperx.transcribe - INFO - Performing transcription...\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
            "It can be re-enabled by calling\n",
            "   >>> import torch\n",
            "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
            "   >>> torch.backends.cudnn.allow_tf32 = True\n",
            "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
            "\n",
            "  warnings.warn(\n",
            "Unable to load any of {libcudnn_cnn.so.9.1.0, libcudnn_cnn.so.9.1, libcudnn_cnn.so.9, libcudnn_cnn.so}\n",
            "Invalid handle. Cannot load symbol cudnnCreateConvolutionDescriptor\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "input = file_name.replace(\":\", \"\\:\").replace(\"'\", \"'\")\n",
        "\n",
        "language_param = \"\"\n",
        "if language != \"auto\":\n",
        "    language_param = f\"--language {language}\"\n",
        "\n",
        "diarize_param = \"\"\n",
        "if assign_speaker_lable:\n",
        "    diarize_param = \"--diarize --hf_token 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\"\n",
        "\n",
        "align_whisper_param = \"\"\n",
        "if align_whisper_output:\n",
        "    align_whisper_param = \"--align_model WAV2VEC2_ASR_LARGE_LV60K_960H\"\n",
        "\n",
        "prompt_param = \"\"\n",
        "if prompt != \"\":\n",
        "    prompt_param = f'--initial_prompt \"{prompt}\"'\n",
        "\n",
        "run = f'whisperx \\'./{input}\\' --model {model_size}{language_param} --output_dir . {prompt_param} {align_whisper_param} {diarize_param}'\n",
        "\n",
        "print(run)\n",
        "\n",
        "!{run}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDuzo5iBSIo7"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "base_filename = os.path.splitext(file_name)[0]\n",
        "base_filename = base_filename.replace(\":\", \"\\:\")\n",
        "srt_filename =f\"{base_filename}.srt\"\n",
        "json_filename = f\"{base_filename}.json\"\n",
        "print(srt_filename)\n",
        "print(json_filename)\n",
        "files.download(srt_filename)\n",
        "files.download(json_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjnDpcfUFs--"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
