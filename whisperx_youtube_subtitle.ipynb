{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "视频教程转化为高质量书籍章节： https://www.answer.ai/posts/2025-10-13-video-to-doc.html 翻译也可以参考其思路\n",
        "\n",
        "\n",
        "idea:  https://x.com/hongming731/status/1980778769163121061   优化翻译流程，将流程Agent化，添加人工校对的部分(将不同模型的翻译结果或者多次翻译结果的diff进行对比，手动在界面选取更好的结果)\n",
        "和大家分享一下我们小组目前在用的这套「人机协同」翻译流程。😀\n",
        "\n",
        "这套流程跑下来，感觉效率和质量平衡得还不错。它不是简单的「AI 翻译 + 人工校对」，而是把 AI 和人的工作做了更细致的拆分：\n",
        "\n",
        "第一步：定规则，AI 打基础\n",
        "\n",
        "一切开始之前，我们先制定了一套比较明确的翻译规则。这非常重要，是后面所有工作的基础，能极大减少后续返工。我们把规则放在了 GitHub 上，大家感兴趣可以看看：https://github.com/ginobefun/agentic-design-patterns-cn/blob/main/rules/rules.md\n",
        "\n",
        "有了规则，我们就让 AI（比如 Gemini 2.5 Pro）先上场，基于原文进行初次翻译。这一步的核心目标是“完整性”，确保所有内容都被翻译了，没有遗漏。\n",
        "\n",
        "第二步：AI 交叉评审，提升稿件下限\n",
        "\n",
        "这是我们流程里比较有意思的一步。初翻稿出来后，我们不会立刻自己上手改，而是会引入另一个 AI 模型（比如 Sonnet 4.5）。\n",
        "\n",
        "我们会让第二个 AI 扮演审稿人的角色，给它我们的翻译规则，让它去交叉评审第一遍的译稿。评审的重点有两个：一是中文表达的地道性，二是长句翻译的优化。\n",
        "\n",
        "为什么这么做？因为不同的 AI 模型有不同的盲点和特长，用一个 AI 去纠正另一个 AI，往往能发现一些人眼容易忽略的生硬表达。跑完这一步，稿件基本就有了七八十分的底子，质量比较稳定了。\n",
        "\n",
        "第三步：人工精校，注入灵魂\n",
        "\n",
        "AI 的工作到此为止，接下来就是人的主场了。\n",
        "\n",
        "我们会进行两到三次的人工阅读和校对。比如我自己的习惯是，会开着中英对照来阅读。虽然 AI 翻译的硬伤不多了，但长句处理上还是会有点生硬。\n",
        "\n",
        "这时候，我个人更倾向于使用「意译」来处理。也就是说，我会跳出原文的句子结构，在理解了作者原意的基础上，用更符合中文阅读习惯的方式把这句话重写出来。这一步是把翻译从「能看懂」提升到「读着顺」、「读着自然」的关键，也是目前 AI 最难替代的地方。\n",
        "\n",
        "第四步：团队终审，确保一致性\n",
        "\n",
        "我们还有一个翻译小组。在个人精校之后，我们会把稿件在小组内进行交叉评审。\n",
        "\n",
        "这一步主要解决的是一致性问题。比如，某个专业术语在第一章和第五章的翻译是否统一？整体的行文风格是否一致？大家会一起讨论，达成共识。\n",
        "\n",
        "等所有章节都翻译完毕，我们还会重新再总校对一次，确保万无一失。\n",
        "\n",
        "最后：部署上线\n",
        "\n",
        "所有工作都完成后，我们会把最终的成品部署成可读性更好的网页，而不是仅仅提供一个文档。\n",
        "\n",
        "总的来说，这套流程里，AI 负责了 70% 的体力活和基础质量保障，而人则专注于那 30% 最有价值的、提升可读性和确保思想准确传达的工作。效果很不错，推荐给大家！"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "o5f3Ppor2p12",
        "outputId": "b4276254-b236-4292-f0bd-d879ac0848bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character '，' (U+FF0C) (ipython-input-1975435615.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1975435615.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    idea:  https://x.com/hongming731/status/1980778769163121061   优化翻译流程，将流程Agent化，添加人工校对的部分(将不同模型的翻译结果或者多次翻译结果的diff进行对比，手动在界面选取更好的结果)\u001b[0m\n\u001b[0m                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '，' (U+FF0C)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ypNCYQbSw3IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fEIwaapOwz71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qXhTiMlsBYPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQYBkyciDzLu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c31505-ff05-4903-f73b-ec4003ed24b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 14 10:14:54 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "#define CUDNN_MAJOR 9\n",
            "#define CUDNN_MINOR 2\n",
            "#define CUDNN_PATCHLEVEL 1\n",
            "--\n",
            "#define CUDNN_VERSION (CUDNN_MAJOR * 10000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n",
            "\n",
            "/* cannot use constexpr here since this is a C-only file */\n"
          ]
        }
      ],
      "source": [
        "#@title **通用参数/Required settings:**\n",
        "!nvidia-smi\n",
        "!nvcc -V\n",
        "!cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR -A 2\n",
        "\n",
        "# @markdown **【IMPORTANT】:**<font size=\"2\">Select uploaded file type.\n",
        "# @markdown **</br>【重要】:** 选择上传的文件类型(视频-video/音频-audio）</font>\n",
        "\n",
        "# encoding:utf-8\n",
        "# file_type = \"audio\"  # @param [\"audio\",\"video\"]\n",
        "\n",
        "# @markdown #### **Youtube video**\n",
        "yt_url = \"https://www.youtube.com/watch?v=DJjZzzPANBY\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown #### **Initial prompt**\n",
        "# @markdown Prompts can be very helpful for correcting specific words or acronyms that the model often misrecognizes in the audio.\n",
        "prompt = \"youtube video:Jordan Fisher is the co-founder & CEO of Standard AI and now leads an AI alignment research team at Anthropic. In his talk at AI Startup School on June 17th, 2025, he frames the future of startups through questions rather than answers—asking how founders should navigate a world where AGI may be just a few years away.\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown #### Model\n",
        "model_size = \"large-v3\"  # @param [\"base\", \"base.en\", \"small\", \"small.en\",\"medium\", \"medium.en\", \"large-v1\",\"large-v2\",\"large-v3\"]\n",
        "\n",
        "# @markdown #### Language\n",
        "language = \"auto\" # @param [\"auto\", \"en\", \"zh\", \"de\", \"es\", \"ru\", \"ko\", \"fr\", \"ja\", \"pt\", \"tr\", \"pl\", \"ca\", \"nl\", \"ar\", \"sv\", \"it\", \"id\", \"hi\", \"fi\", \"vi\", \"he\", \"uk\", \"el\", \"ms\", \"cs\", \"ro\", \"da\", \"hu\", \"ta\", \"no\", \"th\", \"ur\", \"hr\", \"bg\", \"lt\", \"la\", \"mi\", \"ml\", \"cy\", \"sk\", \"te\", \"fa\", \"lv\", \"bn\", \"sr\", \"az\", \"sl\", \"kn\", \"et\", \"mk\", \"br\", \"eu\", \"is\", \"hy\", \"ne\", \"mn\", \"bs\", \"kk\", \"sq\", \"sw\", \"gl\", \"mr\", \"pa\", \"si\", \"km\", \"sn\", \"yo\", \"so\", \"af\", \"oc\", \"ka\", \"be\", \"tg\", \"sd\", \"gu\", \"am\", \"yi\", \"lo\", \"uz\", \"fo\", \"ht\", \"ps\", \"tk\", \"nn\", \"mt\", \"sa\", \"lb\", \"my\", \"bo\", \"tl\", \"mg\", \"as\", \"tt\", \"haw\", \"ln\", \"ha\", \"ba\", \"jw\", \"su\"]\n",
        "\n",
        "# @markdown #### Filename Type\n",
        "# @markdown Use YouTube title as file name by default\n",
        "filename_type = \"id\"  # @param [\"title\", \"id\"]\n",
        "\n",
        "# @markdown #### Assign speaker labels\n",
        "# @markdown Recognize speakers\n",
        "assign_speaker_lable = False # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown #### Align whisper output\n",
        "align_whisper_output = True # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXZPlF99D9jL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6397458b-3fba-44db-9e18-40b25b17a9ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=DJjZzzPANBY\n",
            "[youtube] DJjZzzPANBY: Downloading webpage\n",
            "[youtube] DJjZzzPANBY: Downloading tv client config\n",
            "[youtube] DJjZzzPANBY: Downloading tv player API JSON\n",
            "[youtube] DJjZzzPANBY: Downloading web safari player API JSON\n",
            "[youtube] DJjZzzPANBY: Downloading player 0004de42-main\n",
            "[youtube] DJjZzzPANBY: Downloading m3u8 information\n",
            "[info] DJjZzzPANBY: Downloading 1 format(s): 140-11\n",
            "[download] Sleeping 2.00 seconds as required by the site...\n",
            "[download] Destination: DJjZzzPANBY.m4a\n",
            "[download] 100% of   37.59MiB in 00:00:01 at 28.41MiB/s  \n",
            "[FixupM4a] Correcting container of \"DJjZzzPANBY.m4a\"\n",
            "[ExtractAudio] Destination: DJjZzzPANBY.wav\n",
            "Deleting original file DJjZzzPANBY.m4a (pass -k to keep)\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=DJjZzzPANBY\n",
            "[youtube] DJjZzzPANBY: Downloading webpage\n",
            "[youtube] DJjZzzPANBY: Downloading tv client config\n",
            "[youtube] DJjZzzPANBY: Downloading tv player API JSON\n",
            "[youtube] DJjZzzPANBY: Downloading web safari player API JSON\n",
            "[youtube] DJjZzzPANBY: Downloading m3u8 information\n",
            "视频文件已保存\n",
            "DJjZzzPANBY.wav\n"
          ]
        }
      ],
      "source": [
        "#@title **运行Whisper/Run Whisper**\n",
        "#@markdown 完成后srt文件将自动下载到本地/srt file will be auto downloaded after finish.\n",
        "\n",
        "! pip install yt_dlp\n",
        "\n",
        "print('开始下载视频')\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "import os\n",
        "import subprocess\n",
        "import yt_dlp\n",
        "import torch\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import requests\n",
        "import sys\n",
        "import gc\n",
        "import re\n",
        "\n",
        "# assert file_name != \"\"\n",
        "# assert language != \"\"\n",
        "tic = time.time()\n",
        "\n",
        "file_name = None\n",
        "\n",
        "outtmpl = '%(title)s.%(ext)s'\n",
        "if filename_type == \"id\":\n",
        "    outtmpl = '%(id)s.%(ext)s'\n",
        "\n",
        "ydl_opts = {\n",
        "    'format': 'm4a/bestaudio/best',\n",
        "    'outtmpl': outtmpl,\n",
        "    # ℹ️ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
        "    'postprocessors': [{  # Extract audio using ffmpeg\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'wav',\n",
        "    }]\n",
        "}\n",
        "\n",
        "\n",
        "title = \"no title\"\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    error_code = ydl.download(yt_url)\n",
        "    info = ydl.extract_info(yt_url, download=False)\n",
        "    title = info['title']\n",
        "\n",
        "    info_with_audio_extension = dict(info)\n",
        "    info_with_audio_extension['ext'] = 'wav'\n",
        "    # Return filename with the correct extension\n",
        "    file_name = ydl.prepare_filename(info_with_audio_extension)\n",
        "\n",
        "file_name = file_name.replace(\".m4a\", \".wav\")\n",
        "print('视频文件已保存')\n",
        "print(file_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "print(\"\\n=== 下载 cuDNN 9.1.0 ===\")\n",
        "\n",
        "# 检测 CUDA 版本\n",
        "import subprocess\n",
        "cuda_version_output = subprocess.check_output(\"nvcc --version | grep 'release' | awk '{print $5}'\", shell=True).decode().strip()\n",
        "print(f\"检测到 CUDA 版本: {cuda_version_output}\")\n",
        "\n",
        "# 根据版本选择下载链接\n",
        "if \"12.\" in cuda_version_output:\n",
        "    cudnn_file = \"cudnn-linux-x86_64-9.1.0.70_cuda12-archive.tar.xz\"\n",
        "    # 主下载源\n",
        "    cudnn_url = f\"https://developer.download.nvidia.com/compute/cudnn/9.1.0/local_installers/{cudnn_file}\"\n",
        "    # 备用源（GitHub Release 或其他镜像）\n",
        "    backup_urls = [\n",
        "        \"https://developer.download.nvidia.com/compute/redist/cudnn/v9.1.0/local_installers/12.0/cudnn-linux-x86_64-9.1.0.70_cuda12-archive.tar.xz\",\n",
        "    ]\n",
        "else:\n",
        "    cudnn_file = \"cudnn-linux-x86_64-9.1.0.70_cuda11-archive.tar.xz\"\n",
        "    cudnn_url = f\"https://developer.download.nvidia.com/compute/cudnn/9.1.0/local_installers/{cudnn_file}\"\n",
        "    backup_urls = []\n",
        "\n",
        "print(f\"将下载: {cudnn_file}\")\n",
        "\n",
        "# 尝试下载\n",
        "download_success = False\n",
        "\n",
        "# 方法1: 使用 wget\n",
        "print(\"\\n尝试使用 wget 下载...\")\n",
        "result = os.system(f\"wget --no-check-certificate --timeout=30 -c -O {cudnn_file} {cudnn_url}\")\n",
        "if result == 0 and os.path.exists(cudnn_file):\n",
        "    download_success = True\n",
        "    print(\"✓ wget 下载成功\")\n",
        "\n",
        "# 方法2: 如果 wget 失败，尝试 curl\n",
        "if not download_success:\n",
        "    print(\"\\nwget 失败，尝试使用 curl...\")\n",
        "    os.system(f\"rm -f {cudnn_file}\")  # 删除部分下载\n",
        "    result = os.system(f\"curl -L -o {cudnn_file} {cudnn_url}\")\n",
        "    if result == 0 and os.path.exists(cudnn_file):\n",
        "        download_success = True\n",
        "        print(\"✓ curl 下载成功\")\n",
        "\n",
        "# 方法3: 使用 Python requests\n",
        "if not download_success:\n",
        "    print(\"\\ncurl 失败，尝试使用 Python requests...\")\n",
        "    try:\n",
        "        import requests\n",
        "        response = requests.get(cudnn_url, stream=True, timeout=60)\n",
        "        with open(cudnn_file, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        if os.path.exists(cudnn_file):\n",
        "            download_success = True\n",
        "            print(\"✓ Python requests 下载成功\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Python requests 失败: {e}\")\n",
        "\n",
        "if not download_success:\n",
        "    print(\"\\n❌ 所有下载方法都失败了\")\n",
        "    print(\"\\n备用方案: 使用较旧但稳定的 cuDNN 版本\")\n",
        "    # 下载 cuDNN 8.x（更稳定）\n",
        "    cudnn_file = \"cudnn-linux-x86_64-8.9.7.29_cuda12-archive.tar.xz\"\n",
        "    cudnn_url = f\"https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linux-x86_64/cudnn-linux-x86_64-8.9.7.29_cuda12-archive.tar.xz\"\n",
        "    !wget --no-check-certificate -c -O {cudnn_file} {cudnn_url}\n",
        "\n",
        "# 验证下载的文件\n",
        "print(\"\\n=== 验证下载文件 ===\")\n",
        "!ls -lh /content/*.tar.xz\n",
        "\n",
        "# ============================================\n",
        "# 步骤 3: 解压 cuDNN\n",
        "# ============================================\n",
        "print(\"\\n=== 解压 cuDNN ===\")\n",
        "cudnn_filename = cudnn_url.split(\"/\")[-1]\n",
        "!tar -xf {cudnn_filename}\n",
        "\n",
        "# 获取解压后的目录名\n",
        "cudnn_dir = cudnn_filename.replace(\".tar.xz\", \"\")\n",
        "cudnn_path = f\"/content/{cudnn_dir}\"\n",
        "\n",
        "print(f\"cuDNN 解压到: {cudnn_path}\")\n",
        "\n",
        "# ============================================\n",
        "# 步骤 4: 验证文件\n",
        "# ============================================\n",
        "print(\"\\n=== 验证 cuDNN 文件 ===\")\n",
        "!ls -lh {cudnn_path}/lib/libcudnn_cnn.so*\n",
        "\n",
        "# ============================================\n",
        "# 步骤 5: 设置环境变量\n",
        "# ============================================\n",
        "print(\"\\n=== 设置环境变量 ===\")\n",
        "\n",
        "# 添加到 LD_LIBRARY_PATH\n",
        "lib_path = f\"{cudnn_path}/lib\"\n",
        "current_ld_path = os.environ.get('LD_LIBRARY_PATH', '')\n",
        "\n",
        "if lib_path not in current_ld_path:\n",
        "    os.environ['LD_LIBRARY_PATH'] = f\"{lib_path}:{current_ld_path}\"\n",
        "\n",
        "print(f\"LD_LIBRARY_PATH: {os.environ['LD_LIBRARY_PATH']}\")\n",
        "\n",
        "# 也可以添加到系统路径（可选）\n",
        "os.environ['CUDNN_PATH'] = cudnn_path\n",
        "\n",
        "# ============================================\n",
        "# 步骤 6: 验证 PyTorch 能识别 cuDNN\n",
        "# ============================================\n",
        "print(\"\\n=== 验证 PyTorch 和 cuDNN ===\")\n",
        "\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"cuDNN enabled: {torch.backends.cudnn.enabled}\")\n",
        "print(f\"cuDNN version: {torch.backends.cudnn.version()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB1jFGyx0HVX",
        "outputId": "b4cda646-0952-461f-cefc-c3215fc66773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 下载 cuDNN 9.1.0 ===\n",
            "检测到 CUDA 版本: 12.5,\n",
            "将下载: cudnn-linux-x86_64-9.1.0.70_cuda12-archive.tar.xz\n",
            "\n",
            "尝试使用 wget 下载...\n",
            "\n",
            "wget 失败，尝试使用 curl...\n",
            "✓ curl 下载成功\n",
            "\n",
            "=== 验证下载文件 ===\n",
            "-rw-r--r-- 1 root root 10 Oct 14 08:26 /content/cudnn-linux-x86_64-9.1.0.70_cuda12-archive.tar.xz\n",
            "\n",
            "=== 解压 cuDNN ===\n",
            "tar: This does not look like a tar archive\n",
            "xz: (stdin): File format not recognized\n",
            "tar: Child returned status 1\n",
            "tar: Error is not recoverable: exiting now\n",
            "cuDNN 解压到: /content/cudnn-linux-x86_64-9.1.0.70_cuda12-archive\n",
            "\n",
            "=== 验证 cuDNN 文件 ===\n",
            "ls: cannot access '/content/cudnn-linux-x86_64-9.1.0.70_cuda12-archive/lib/libcudnn_cnn.so*': No such file or directory\n",
            "\n",
            "=== 设置环境变量 ===\n",
            "LD_LIBRARY_PATH: /content/cudnn-linux-x86_64-9.1.0.70_cuda12-archive/lib:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/lib64-nvidia\n",
            "\n",
            "=== 验证 PyTorch 和 cuDNN ===\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "CUDA version: 12.6\n",
            "cuDNN enabled: True\n",
            "cuDNN version: 91002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e8fd8a-d952-43c0-c768-e24a4f3171a7",
        "id": "c9q8h2OfyhNh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/m-bain/whisperx\n",
            "  Cloning https://github.com/m-bain/whisperx to /tmp/pip-req-build-jryrrltu\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/m-bain/whisperx /tmp/pip-req-build-jryrrltu\n",
            "  Resolved https://github.com/m-bain/whisperx to commit 505bd9c0b522674e3782f3393644e3f2d7d238ba\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ctranslate2>=4.5.0 (from whisperx==3.7.2)\n",
            "  Downloading ctranslate2-4.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting faster-whisper>=1.1.1 (from whisperx==3.7.2)\n",
            "  Downloading faster_whisper-1.2.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: nltk>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from whisperx==3.7.2) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from whisperx==3.7.2) (2.0.2)\n",
            "Collecting pandas<2.3.0,>=2.2.3 (from whisperx==3.7.2)\n",
            "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting av<16.0.0 (from whisperx==3.7.2)\n",
            "  Downloading av-15.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting pyannote-audio<4.0.0,>=3.3.2 (from whisperx==3.7.2)\n",
            "  Downloading pyannote_audio-3.4.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from whisperx==3.7.2) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (from whisperx==3.7.2) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.48.0 in /usr/local/lib/python3.12/dist-packages (from whisperx==3.7.2) (4.57.0)\n",
            "Requirement already satisfied: triton>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from whisperx==3.7.2) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from ctranslate2>=4.5.0->whisperx==3.7.2) (75.2.0)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.12/dist-packages (from ctranslate2>=4.5.0->whisperx==3.7.2) (6.0.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper>=1.1.1->whisperx==3.7.2) (0.35.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper>=1.1.1->whisperx==3.7.2) (0.22.1)\n",
            "Collecting onnxruntime<2,>=1.14 (from faster-whisper>=1.1.1->whisperx==3.7.2)\n",
            "  Downloading onnxruntime-1.23.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from faster-whisper>=1.1.1->whisperx==3.7.2) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9.1->whisperx==3.7.2) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9.1->whisperx==3.7.2) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9.1->whisperx==3.7.2) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0,>=2.2.3->whisperx==3.7.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0,>=2.2.3->whisperx==3.7.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0,>=2.2.3->whisperx==3.7.2) (2025.2)\n",
            "Collecting asteroid-filterbanks>=0.4 (from pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (0.8.1)\n",
            "Collecting lightning>=2.0.1 (from pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading lightning-2.5.5-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (2.3.0)\n",
            "Collecting pyannote.core<6.0,>=5.0.0 (from pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyannote.database<6.0,>=5.0.1 (from pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pyannote.metrics<4.0,>=3.2 (from pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting pyannote.pipeline<4.0,>=3.0.1 (from pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n",
            "Collecting pytorch_metric_learning>=2.1.0 (from pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading pytorch_metric_learning-2.9.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (13.9.4)\n",
            "Collecting semver>=3.0.0 (from pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (0.13.1)\n",
            "Collecting speechbrain>=1.0.0 (from pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tensorboardX>=2.6 (from pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting torch_audiomentations>=0.11.0 (from pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting torchmetrics>=0.11.0 (from pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.7.1->whisperx==3.7.2) (1.11.1.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.48.0->whisperx==3.7.2) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.48.0->whisperx==3.7.2) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.48.0->whisperx==3.7.2) (0.6.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13->faster-whisper>=1.1.1->whisperx==3.7.2) (1.1.10)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting pytorch-lightning (from lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading pytorch_lightning-2.5.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf<3.0,>=2.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (4.9.3)\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper>=1.1.1->whisperx==3.7.2)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper>=1.1.1->whisperx==3.7.2) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper>=1.1.1->whisperx==3.7.2) (5.29.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from pyannote.core<6.0,>=5.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (2.4.0)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.core<6.0,>=5.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (1.16.2)\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.database<6.0,>=5.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (0.19.2)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.metrics<4.0,>=3.2->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (1.6.1)\n",
            "Collecting docopt>=0.6.2 (from pyannote.metrics<4.0,>=3.2->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.12/dist-packages (from pyannote.metrics<4.0,>=3.2->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pyannote.metrics<4.0,>=3.2->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (3.10.0)\n",
            "Collecting optuna>=3.1 (from pyannote.pipeline<4.0,>=3.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=2.2.3->whisperx==3.7.2) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (2.19.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (2.0.0)\n",
            "Collecting hyperpyyaml (from speechbrain>=1.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from speechbrain>=1.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (0.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.7.1->whisperx==3.7.2) (1.3.0)\n",
            "Collecting julius<0.3,>=0.2.3 (from torch_audiomentations>=0.11.0->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-pitch-shift>=1.2.2 (from torch_audiomentations>=0.11.0->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.7.1->whisperx==3.7.2) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.48.0->whisperx==3.7.2) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.48.0->whisperx==3.7.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.48.0->whisperx==3.7.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.48.0->whisperx==3.7.2) (2025.10.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (2.23)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (3.13.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.0.0->pyannote.metrics<4.0,>=3.2->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (3.2.5)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.1->pyannote.pipeline<4.0,>=3.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (1.16.5)\n",
            "Collecting colorlog (from optuna>=3.1->pyannote.pipeline<4.0,>=3.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.1->pyannote.pipeline<4.0,>=3.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (2.0.43)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics<4.0,>=3.2->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (3.6.0)\n",
            "Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch_audiomentations>=0.11.0->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.12.1->pyannote.database<6.0,>=5.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (1.5.4)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper>=1.1.1->whisperx==3.7.2)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=1.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading ruamel.yaml-0.18.15-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (1.22.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline<4.0,>=3.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (1.3.10)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2)\n",
            "  Downloading ruamel.yaml.clib-0.2.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline<4.0,>=3.0.1->pyannote-audio<4.0.0,>=3.3.2->whisperx==3.7.2) (3.2.4)\n",
            "Downloading av-15.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faster_whisper-1.2.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m140.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote_audio-3.4.0-py2.py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.8/897.8 kB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
            "Downloading lightning-2.5.5-py3-none-any.whl (828 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m828.5/828.5 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m125.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n",
            "Downloading pytorch_metric_learning-2.9.0-py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semver-3.0.4-py3-none-any.whl (17 kB)\n",
            "Downloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading pytorch_lightning-2.5.5-py3-none-any.whl (832 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m832.4/832.4 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
            "Downloading ruamel.yaml-0.18.15-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading ruamel.yaml.clib-0.2.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (753 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.1/753.1 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: whisperx, docopt, julius\n",
            "  Building wheel for whisperx (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisperx: filename=whisperx-3.7.2-py3-none-any.whl size=16485187 sha256=b93c9e78c6a4a02b132e0aeaaaa9404f84820ef8a8206bc93156789d152b8da4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-81ef5ofb/wheels/6f/a8/45/a2a85135519ce866abd923a801ccdb985291743cd6b73e9b6d\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=921e728c16dd2655310a383a4c49c9b85fda087be6867910f4e53e125520868a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=a61941d6124ce192173c1d1a923dabcc93e1c465582e5ec936cdd2f4c1fb144b\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c1/ca/544dafe48401e8e2e17064dfe465a390fca9e8720ffa12e744\n",
            "Successfully built whisperx docopt julius\n",
            "Installing collected packages: primePy, docopt, tensorboardX, semver, ruamel.yaml.clib, lightning-utilities, humanfriendly, ctranslate2, colorlog, av, ruamel.yaml, pyannote.core, pandas, coloredlogs, optuna, onnxruntime, hyperpyyaml, torchmetrics, pytorch_metric_learning, pyannote.database, julius, faster-whisper, asteroid-filterbanks, torch-pitch-shift, speechbrain, pytorch-lightning, pyannote.pipeline, pyannote.metrics, torch_audiomentations, lightning, pyannote-audio, whisperx\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asteroid-filterbanks-0.4.0 av-15.1.0 coloredlogs-15.0.1 colorlog-6.9.0 ctranslate2-4.6.0 docopt-0.6.2 faster-whisper-1.2.0 humanfriendly-10.0 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.5.5 lightning-utilities-0.15.2 onnxruntime-1.23.1 optuna-4.5.0 pandas-2.2.3 primePy-1.3 pyannote-audio-3.4.0 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-lightning-2.5.5 pytorch_metric_learning-2.9.0 ruamel.yaml-0.18.15 ruamel.yaml.clib-0.2.14 semver-3.0.4 speechbrain-1.0.3 tensorboardX-2.6.4 torch-pitch-shift-1.2.5 torch_audiomentations-0.12.0 torchmetrics-1.8.2 whisperx-3.7.2\n",
            "Collecting ctranslate2==4.4.0\n",
            "  Downloading ctranslate2-4.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from ctranslate2==4.4.0) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ctranslate2==4.4.0) (2.0.2)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.12/dist-packages (from ctranslate2==4.4.0) (6.0.3)\n",
            "Downloading ctranslate2-4.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ctranslate2\n",
            "  Attempting uninstall: ctranslate2\n",
            "    Found existing installation: ctranslate2 4.6.0\n",
            "    Uninstalling ctranslate2-4.6.0:\n",
            "      Successfully uninstalled ctranslate2-4.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "whisperx 3.7.2 requires ctranslate2>=4.5.0, but you have ctranslate2 4.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ctranslate2-4.4.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libcudnn8 libcudnn8-dev\n",
            "0 upgraded, 2 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 885 MB of archives.\n",
            "After this operation, 2,380 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcudnn8 8.9.7.29-1+cuda12.2 [444 MB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcudnn8-dev 8.9.7.29-1+cuda12.2 [440 MB]\n",
            "Fetched 885 MB in 20s (43.6 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libcudnn8.\n",
            "(Reading database ... 126675 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.9.7.29-1+cuda12.2_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.9.7.29-1+cuda12.2) ...\n",
            "Selecting previously unselected package libcudnn8-dev.\n",
            "Preparing to unpack .../libcudnn8-dev_8.9.7.29-1+cuda12.2_amd64.deb ...\n",
            "Unpacking libcudnn8-dev (8.9.7.29-1+cuda12.2) ...\n",
            "Setting up libcudnn8 (8.9.7.29-1+cuda12.2) ...\n",
            "Setting up libcudnn8-dev (8.9.7.29-1+cuda12.2) ...\n",
            "update-alternatives: warning: forcing reinstallation of alternative /usr/include/x86_64-linux-gnu/cudnn_v9.h because link group libcudnn is broken\n",
            "update-alternatives: using /usr/include/x86_64-linux-gnu/cudnn_v8.h to provide /usr/include/cudnn.h (libcudnn) in manual mode\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Uninstall existing whisperx and ctranslate2\n",
        "# !pip uninstall -y whisperx ctranslate2\n",
        "\n",
        "# Ensure LD_LIBRARY_PATH is set correctly for CUDA libraries\n",
        "# This is often necessary for packages that link against CUDA/cuDNN\n",
        "# Get the CUDA installation path from the system\n",
        "# cuda_path = !which nvcc\n",
        "# if cuda_path:\n",
        "#     cuda_lib_path = os.path.join(os.path.dirname(os.path.dirname(cuda_path[0])), 'lib64')\n",
        "#     os.environ['LD_LIBRARY_PATH'] = f\"{cuda_lib_path}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
        "#     print(f\"Updated LD_LIBRARY_PATH: {os.environ['LD_LIBRARY_PATH']}\")\n",
        "# else:\n",
        "#     print(\"Could not find nvcc to determine CUDA library path.\")\n",
        "\n",
        "\n",
        "# Install specific cuDNN version (9.1.0) that might be required by whisperx\n",
        "# We will attempt to install libcudnn9-cuda-12 version 9.1.0 if available.\n",
        "# print(\"Attempting to install libcudnn9-cuda-12=9.1.0 and libcudnn9-dev-cuda-12=9.1.0...\")\n",
        "# !sudo apt update\n",
        "# Use --allow-unauthenticated and --allow-downgrades if necessary, but be cautious\n",
        "# Pin the version to 9.1.0\n",
        "# !sudo apt-get install -y --allow-unauthenticated --allow-downgrades libcudnn9-cuda-12=9.1.0 libcudnn9-dev-cuda-12=9.1.0\n",
        "\n",
        "# Reinstall whisperx\n",
        "!pip install git+https://github.com/m-bain/whisperx\n",
        "!pip install ctranslate2==4.4.0\n",
        "!sudo apt-get install -y --allow-unauthenticated --allow-downgrades libcudnn8 libcudnn8-dev\n",
        "\n",
        "# Verify ctranslate2 version after whisperx installation\n",
        "# whisperx requires a specific range of ctranslate2 versions\n",
        "# !pip show ctranslate2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"--- PyTorch GPU/cuDNN Verification ---\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✅ Success: PyTorch can access CUDA.\")\n",
        "    print(f\"   - CUDA Version (from PyTorch): {torch.version.cuda}\")\n",
        "    if torch.backends.cudnn.is_available():\n",
        "        print(f\"✅ Success: cuDNN is available.\")\n",
        "        print(f\"   - cuDNN Version (from PyTorch): {torch.backends.cudnn.version()}\")\n",
        "        try:\n",
        "             # 将一个需要 cuDNN 的卷积层移动到GPU上\n",
        "            _ = torch.nn.Conv2d(1, 32, 3).to('cuda')\n",
        "            print(\"✅ cuDNN check passed: A convolutional layer was successfully moved to the GPU.\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failure: An error occurred during cuDNN check: {e}\")\n",
        "    else:\n",
        "        print(\"❌ Failure: cuDNN is not available to PyTorch.\")\n",
        "else:\n",
        "    print(\"❌ Failure: PyTorch cannot access CUDA. Please check installation and runtime type.\")"
      ],
      "metadata": {
        "id": "iWalh0oTlWPb",
        "outputId": "b2ccefff-b8a8-4603-ef10-c2517a94e253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- PyTorch GPU/cuDNN Verification ---\n",
            "✅ Success: PyTorch can access CUDA.\n",
            "   - CUDA Version (from PyTorch): 12.6\n",
            "✅ Success: cuDNN is available to PyTorch.\n",
            "   - cuDNN Version (from PyTorch): 91002\n",
            "✅ cuDNN check passed: A convolutional layer was successfully moved to the GPU.\n",
            "--- System cuDNN Version ---\n",
            "#define CUDNN_MAJOR 9\n",
            "#define CUDNN_MINOR 2\n",
            "#define CUDNN_PATCHLEVEL 1\n",
            "--\n",
            "#define CUDNN_VERSION (CUDNN_MAJOR * 10000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n",
            "\n",
            "/* cannot use constexpr here since this is a C-only file */\n",
            "\n",
            "✅ Success: whisperx command is available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1rMKyCdEvj9",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30e5e9a2-662d-4718-c5e1-83ff98b22ee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whisperx './DJjZzzPANBY.wav' --model large-v3 --output_dir . --initial_prompt \"youtube video:Jordan Fisher is the co-founder & CEO of Standard AI and now leads an AI alignment research team at Anthropic. In his talk at AI Startup School on June 17th, 2025, he frames the future of startups through questions rather than answers—asking how founders should navigate a world where AGI may be just a few years away.\" --align_model WAV2VEC2_ASR_LARGE_LV60K_960H \n",
            "2025-10-14 10:19:18.690882: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760437158.710731    4491 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760437158.716963    4491 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760437158.733715    4491 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760437158.733742    4491 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760437158.733745    4491 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760437158.733749    4491 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-14 10:19:18.738442: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/audio/core/io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  torchaudio.list_audio_backends()\n",
            "/usr/local/lib/python3.12/dist-packages/speechbrain/utils/torch_audio_backend.py:57: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  available_backends = torchaudio.list_audio_backends()\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n",
            "tokenizer.json: 0.00B [00:00, ?B/s]\n",
            "vocabulary.json: 1.07MB [00:00, 16.5MB/s]\n",
            "\n",
            "config.json: 0.00B [00:00, ?B/s]\u001b[A\n",
            "\n",
            "preprocessor_config.json: 100% 340/340 [00:00<00:00, 2.71MB/s]\n",
            "config.json: 2.39kB [00:00, 1.45MB/s]\n",
            "tokenizer.json: 2.48MB [00:00, 33.5MB/s]\n",
            "model.bin: 100% 3.09G/3.09G [00:59<00:00, 51.7MB/s]\n",
            "2025-10-14 10:20:42 - whisperx.asr - INFO - No language specified, language will be detected for each audio file (increases inference time)\n",
            "2025-10-14 10:20:42 - whisperx.vads.pyannote - INFO - Performing voice activity detection using Pyannote...\n",
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../usr/local/lib/python3.12/dist-packages/whisperx/assets/pytorch_model.bin`\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.8.0+cu126. Bad things might happen unless you revert torch to 1.x.\n",
            "2025-10-14 10:20:48 - whisperx.transcribe - INFO - Performing transcription...\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
            "It can be re-enabled by calling\n",
            "   >>> import torch\n",
            "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
            "   >>> torch.backends.cudnn.allow_tf32 = True\n",
            "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
            "\n",
            "  warnings.warn(\n",
            "2025-10-14 10:20:54 - whisperx.asr - INFO - Detected language: en (1.00) in first 30s of audio\n",
            "Transcript: [0.031 --> 21.209]  Welcome to this talk. I'm extremely confused. I think maybe more confused than I've ever been in my entire life. Probably, definitely more confused than I've ever been in my entire life. And I want to talk about that because I think, I don't know, when you're confused, that's the start of something interesting. If you're trained as a scientist, that's like the thing you want to pay most attention to is that\n",
            "Transcript: [21.209 --> 48.057]  I've been in technology for like my whole life and I feel like I've always had this advantage where it's like, oh, I understand what's happening. I actually kind of think I know what's going to happen over the next five or 10 years. And I've used that to my advantage. Like I've planned my career around it. I've founded companies around it. You know, I've kind of gotten ahead of the trend and gotten involved at the right time in a lot of different places. And that's like really been\n",
            "Transcript: [48.057 --> 65.287]  Great for me. I no longer feel that way, though. Like, I can't see five years into the future. I can see like three weeks or less. And I really just don't know what's going to happen. So I want to have a series of questions that hopefully help me\n",
            "Transcript: [65.287 --> 92.388]  Maybe it'll help you all figure out what's going on. Maybe not. Not all questions are useful. But I think asking good questions is extremely important to running a startup, running a research team, running your life. Whatever you're doing, I think we all need to stop sometimes and just ask questions. But I think the moment in time that we're going through right now with AI is extremely challenging and fast. And if there's ever a time to stop and ask questions, it's probably right now.\n",
            "Transcript: [92.388 --> 107.44]  All views are my own, obviously. I think we're required to say that. My day job is running an alignment research team at Anthropic. I've also been through YC, I've done multiple startups in my life, so maybe my perspective on\n",
            "Transcript: [107.44 --> 127.91]  AI plus startups is useful for you all if you're thinking about startup life or AI life. Okay, that's enough of a preamble. Let's jump in. This is sort of like the main question that I want to ask. Everything's changing. How should that impact everything about my life? Honestly, like, should you even start a startup? Like, it's a big question.\n",
            "Transcript: [127.91 --> 152.969]  Let's assume that you're starting a startup or you're running a startup already. How should it impact your strategy? How should it impact your product? How should it impact how you're building your team? These are big, open questions. And I think AI is probably going to change how you answer them today and probably differently tomorrow. And there's this kind of paradox that I've seen throughout my career running startups and talking to startup founders. Everyone always tells you focus is everything. That's the advantage that startups have.\n",
            "Transcript: [152.969 --> 179.328]  Focus, focus, focus. Big companies can't focus. That's why you can outcompete them and run circles around them. But despite the fact that focus is everything, the other truth of running a startup is you have to focus on everything, right? Like it's your job to think about hiring and fundraising and product and strategy and go to market and just like everything, right? And then all of a sudden, in the middle of like a product launch, someone on your team quits and someone else threatens to quit. And it's just like, it's madness and it's happening all the time.\n",
            "Transcript: [179.328 --> 208.994]  This is just an extreme paradox, but I think it kind of makes founders suitable to this biggest question that we're all facing as a society, which is what's going to happen with AI and what should we be doing? Because founders are the people that have to just answer every single question always. So I think it's a good positioning. There's been sort of like common, quote unquote common, just like in the last six months, best practices, like what do you think about AI for your product? And people say like, oh, you should like think about what happens over the next six months.\n",
            "Transcript: [208.994 --> 230.999]  Think about what the next foundation models are going to be able to do and make sure you're planning your product, anticipating what those capabilities are going to do. Don't plan for the capabilities of today. I think it's extremely valid advice. You should definitely take it seriously. But I want to up the ante a little bit and say, actually, you should be planning two years in advance because it's extremely likely that we will have AGI in the next few years.\n",
            "Transcript: [230.999 --> 252.481]  Maybe it's not two, maybe it's three, right? But I think you should be planning your company and your strategy around this fact, right? And like, there's extreme uncertainty, right? So don't like take it too seriously. Don't have like literally a two year plan. But if you're not thinking a little bit about how this is going to change everything from hiring to marketing to go to market, et cetera, I don't think you're doing your job as a founder.\n",
            "Transcript: [252.987 --> 273.22]  OK, so that's really the theme here today is I want to go through a series of questions and take this lens of AGI arriving and also just take the lens of what's changing near term in AI over the next six months. I think there's been a lot of conversations around like, oh, actually, the impact of AI is going to be slower than we think. And the reason is that big companies suck and they have to like\n",
            "Transcript: [273.22 --> 288.391]  They take a lot of time to buy things, right? They don't realize the trend. And then, you know, the enterprise sales cycle is really slow. So all the Fortune 500 companies are just going to take years and years to digest all the SaaS products that you all might want to be building. And I think that's actually extremely nearsighted.\n",
            "Transcript: [288.391 --> 312.05]  Because what's going to happen, I think, open question is the buy side, the enterprises, they're going to get armed with AGI or strong agents over the next couple of years as well, right? And that's not just via the SaaS products that they might be building. It'll just be natively inside, right? Their teams are going to be using the next versions of LLMs to make buying decisions, right? And to figure out how to accelerate their adoption cycle.\n",
            "Transcript: [312.05 --> 334.46]  The force of AI is not just on this like product revolution that the startups are building, it's also on the buy side, right? And I think this is like an interesting, weird thing about AI is that it's going to rise, you know, the water rises and all ships rise with it. It's not just the startups, the incumbents benefit from AI too. And I think we're seeing that in other places too, where large enterprises sometimes aren't even going out and buying\n",
            "Transcript: [334.46 --> 363.029]  Software from SaaS providers anymore. They're like, I can just like throw two people at Cloud Code and they'll build it and it'll be dedicated to the capabilities that I need for my organization. So buy side is going to be evolving really quickly. And I think it's an open question. What does that mean? Right. I think another angle on this is, you know, when you have AI powered outbound sales, we think about what that is going to do to the marketplace. But there's going to be this in this buy side, too, of like receiving those sale calls from AIs and trying to parse what's going on.\n",
            "Transcript: [363.029 --> 386.907]  The dynamics aren't really clear how it's going to play out. This is a kind of related question, like is software going to fully commoditize? Is it even going to make sense to run a SaaS provider in like two years or three years? Or is it like going to be the case that enterprises really do just build all their software in-house because it's just one, you know, one prompt to cloud code, the next gen version of cloud code, right? And actually you just need in-house product managers and they're going to do everything for you.\n",
            "Transcript: [386.907 --> 405.318]  Like that's one real outcome. That could happen to the consumer side too. Like maybe consumers eventually are just like not downloading apps anymore. They're just building apps on demand for themselves. They don't even think about it that way. Like they don't even think about them as apps anymore. It's just, yeah, I want my phone to do something for me. I ask it and yeah, it made an app for me. Sure. What's the point of downloading an app at this point?\n",
            "Transcript: [405.318 --> 426.833]  So that's one outcome, but I think another outcome actually is the opposite. It's like, maybe all of this automation on generating code makes it easier to just raise the quality bar extremely high. And sure, you can make the equivalent of today's apps very easily just by prompting, but can you make an exceptional app tomorrow, the equivalent of a great team that's working with AI, to raise the bar?\n",
            "Transcript: [426.833 --> 441.768]  I don't actually know the answer to this question, but I think it might be different depending on the vertical, and you should be thinking about this for your product. I think it's an interesting question, too. Like, if you can write software on demand, do you even make apps anymore? Or is it really on demand, right?\n",
            "Transcript: [442.055 --> 463.267]  We're taking cloud code and things like it to ahead of time figure out what a user needs and then build as much software as we need to support all those use cases. But if you can do code on demand, why not do it on demand? You know, on the fly, here's this user, they're doing something in your app potentially, and the app realizes that the app can't support what they want. And on demand, you generate code for that user.\n",
            "Transcript: [463.267 --> 490.013]  I think this is a really interesting pattern if you can make it work, but it raises all sorts of issues around like trust, right? It's one thing to do generative UI where maybe you're changing the shape of the interface for the user, but like real new behaviors would require going down to the database level, right? Or the backend. And if you want your AI to be able to do that on demand, you better trust that that AI can do its job, right? And obviously right now, AIs are not trustable enough to make that happen. So I think trust is going to be a big theme over how these different questions play out.\n",
            "Transcript: [490.25 --> 516.912]  Similar question, like, what should UI look like? You know, I think people talk about generative UI. It hasn't really happened yet, but I'm, like, bullish to see when it does happen. Is an on-demand UI the right interface, or is there something even more different that we haven't thought about? I think about this specifically in the context of multimodality, weaving together auditory and images and video and text, and what's the right input back from the user? You know, as a user, sometimes I want to speak, sometimes I want to use touch interfaces.\n",
            "Transcript: [516.912 --> 543.794]  It's really contextual, depending on whether I'm in a crowded area, et cetera, right? I think you want to meet the user where they are. Like, what's the easiest thing? What's the easiest way for them to interface with your product right now? That's the question you're thinking about. And I think all this ties into this big question that I have, which is like, we have all these products today that have great distribution and people realize that we need to insert AI into it. All the major players are doing this, right? Slapping like a chatbot on, slapping on some like agentic behavior or whatever, right?\n",
            "Transcript: [543.794 --> 564.01]  They're like retrofitting their products to try to enable something with AI, right? But that's like trying to like sprinkle pixie dust on something. It's natural to think it's better to build it a new product from the ground up that's AI native. Like that's that's like my startup mentality, like new technology revolution. You need to start from scratch if you want to build this right. But that might not be true.\n",
            "Transcript: [564.01 --> 578.692]  It actually might be the case that retrofitting existing products with the benefit of distribution wins. And I think, again, it might not be universal. This might be vertical by vertical. But I think this is a really important question that you should be asking yourself. And don't just have an opinion.\n",
            "Transcript: [578.894 --> 601.287]  Figure out the causal mechanisms that allow you to validate your hypothesis. Because I think these types of questions will make or break different products. Okay, so that's a lot about products, product strategy, but I think every part of a startup is extremely important. The most important thing is your culture, it's your team. Will team sizes get even smaller? I think that's the default that people are assuming.\n",
            "Transcript: [601.287 --> 617.808]  But I think similar to products where you can retrofit or you can build from scratch, I think there's going to be this question which is, are AI-native teams that were built from scratch to be AI-native, are they going to have some advantage over large companies that are downsizing and finding ways to make themselves more efficient with AI, right?\n",
            "Transcript: [617.808 --> 638.851]  Are there like team patterns where you kind of like just operate differently if you started from an AI native place? And actually that might be different every 6, 12 or 18 months, right? Because the capabilities of AI are changing. So like an AI native company today might be different than what an AI native company tomorrow looks like in 12 months. You might be outdated if you're not like thinking about how to retrofit yourself.\n",
            "Transcript: [638.851 --> 659.742]  Like I said, I think trust is going to be extremely important, right? So how does the security model change? I mentioned this already in the case of on-demand code where you want your LLM to be able to go all the way back down to the database layer and do something on demand for that customer, for that consumer. But you can't do that if you can't trust the controls that you have in place and if you can't trust the capabilities of the model to do the right thing, right?\n",
            "Transcript: [659.742 --> 677.495]  I think about this also in the context of assistance. The whole point of an assistant is to be useful to the consumer, to that user. But we have all these walled gardens, and you're starting to see maybe you need different agents in different settings, right? But that's not what you want. As a user, you want one agent for all of your things, right?\n",
            "Transcript: [677.495 --> 696.209]  So me, I want my personal agent and my professional agent at work to be able to work together, right? That raises all sorts of concerns, like there's stuff that I do on my personal time or you do on your personal time that maybe you don't want your employer to know about, right? So how do you make sure that the information is segregated while still allowing those agents to collaborate in the right way?\n",
            "Transcript: [696.378 --> 719.007]  Those are hard questions. Can you do it, right? Can you get to the same level of usefulness? Or do you have to compromise somehow on some of these security aspects? A deeper question about agents is, you know, we think about alignment as, can we trust the AI? But the truth is, even if you have a perfectly aligned, you know, quote unquote, like intent aligned model,\n",
            "Transcript: [719.007 --> 735.747]  It's going to be used by a corporation or a startup to build an agent for that user. And then the question is, can you trust the startup that's building the agent, right? Is this agent that they've developed for you actually acting on your behalf? If this is like an ad based company,\n",
            "Transcript: [735.747 --> 764.418]  And you're using this agent to search for like a, you know, new brand of shoes or whatever, right? Like, is it going to be biased? Is it going to be like pumping you towards one direction? That's not what you want as the user, right? But it might be what you want as the corporation. So and then how do we start even asking these questions, right? I think going back to the previous slide around like personal versus professional agents, I really care if I'm using that agent, I really care that it's operating on my behalf, right? If it's like secretly operating on behalf of my company,\n",
            "Transcript: [764.418 --> 781.698]  and it's like optimizing things in my personal life that's better for the company than me that's like extremely scary and i think it gets more scary the more capable the models get so i think it's important to start thinking about trust right like how do we how do we instill trust uh not just in the models but in the agents and the companies that are building these agents right\n",
            "Transcript: [781.883 --> 805.846]  And I think ultimately what that really comes down to is trust in you all, or trust in the companies that you're building, right? How can a user trust this company that's building this product? Especially, you know, already today we have these extremely small teams. Tomorrow the teams might be even smaller. You might have these like semi-automated teams almost. How can you trust that team? You might say, well, I don't know, like, we trust companies today.\n",
            "Transcript: [805.846 --> 834.584]  One of the core reasons we trust companies today is because they're composed of a diversity of people. You can trust to some extent that if there's reasonable culture at a company, that if the company decides, if the CEO decides to do something bad, that someone in the company is going to raise their hand and say, no, I don't support this. I'm going to whistleblow. I'm going to leak this. I'm going to quit. I'm going to take a bunch of people with me. I'm pissed. I hate this, right? And without the people supporting the company, the company doesn't have a product, right?\n",
            "Transcript: [834.584 --> 860.47]  But in a semi-automated world, that's no longer true. And it could be the fact that a single person could make a decision that changes the entire impact of a product. And there's no single person that might be aware of that except themselves. It gets extremely easy for bad actors to do bad things. And if you've followed along the history of Silicon Valley or the history of humanity, the truth is a huge majority of people are misaligned, especially when money is on the line.\n",
            "Transcript: [860.96 --> 885.817]  So I think this is a really important question. And the truth is, we already think about this. Large enterprises, for example, already distrust startups, partially for this reason. There's a lot of reasons why large enterprises distrust startups. One of them is they're worried they're going to go out of business tomorrow, right? But it's just a lot easier for a small startup to do the wrong thing compared to a big company, right? That's part of what makes small startups successful sometimes. So I think this is already on the top of minds of enterprises, but it'll increasingly be on the top of minds of\n",
            "Transcript: [885.817 --> 910.488]  So that kind of leads me to this next question, which is, well, how do we instill trust? What does a new set of guardrails need to look like? If you don't have all these human guardrails inside of your company, where you've worked really hard to build great culture and build a collection of people who care about doing the right thing, if that's not the thing that makes a company ethical anymore, how else can we make sure that there are guardrails?\n",
            "Transcript: [910.977 --> 934.501]  What does that look like? A lot of people have been throwing around ideas on this. There's this notion of AI-powered auditing. What should an audit look like in an AGI world? There's this huge advantage that AI has over humans when it comes to bringing comfort to an audit, which is that they can be less biased, and they can also have no memory. For an example, if you agree as a company to let an AI audit you,\n",
            "Transcript: [934.501 --> 950.161]  And you can say, hey, if you don't find any malfeasance, if after your audit you've decided that we didn't do anything wrong as we claimed, maybe you have like a public mission statement that the AI is confirming, then the AI deletes itself, right? All of its notes, et cetera, get deleted.\n",
            "Transcript: [950.296 --> 979.253]  And that's a big advantage over having human auditors where they could take information with them, right? There's already auditing happening today, right? For legal reasons, for financial reasons, you know, or you want to be like certified organic or whatever it is, right? But you have to let people come into your company and audit you. And there's danger there, right? They might take with them like IP or they might find sensitive things and then like, you know, which were unrelated to the thing you wanted to get audited. And all these bad things can happen, right? So there's like this potential, I think, for\n",
            "Transcript: [979.253 --> 996.01]  AI to give us a much stronger auditing system. That can be part of how we build trust, how we let people build trust in us and the companies and products that we're building. So that kind of leads me to this next question, which is, should we be doing that? Like, are there some other ways that you plan on having\n",
            "Transcript: [996.01 --> 1020.985]  What is the future of AGI?\n",
            "Transcript: [1020.985 --> 1048.002]  Maybe this isn't the right way to do it, but I think things like this are going to be possible soon, but not possible today. And they might be the flavor of things that we need to start building trust once we're in this world where we've lost a lot of trust.\n",
            "Transcript: [1048.306 --> 1065.535]  Related question around alignment. So I think a lot about alignment and I think there's this question of like, what parts of alignment do we have to solve? You know, one reason we're working on alignment is because of the control issue. We want to make sure AIs, you know, stay under the control of humans. But I think there's this like extremely\n",
            "Transcript: [1065.535 --> 1092.839]  High pressure question for the next 12 months, which is what parts of alignment do we have to solve just to make these models more economically viable, right? Just to make sure that the agents that all the startups are building, as their horizons get longer and longer, that we can actually trust those agents are like not going off the rails, right? And if you use Claude code today and it works like five minutes at a time for you, that's like one thing because you're going to review a lot of what Claude does. But if you're going to trust an LLM to work for a day at a time or a week at a time before you intervene,\n",
            "Transcript: [1092.839 --> 1112.802]  You better have some degree of certainty that it's not going completely off the rails, right? So I think this is like a huge open question. I'm actually really positive and bullish that there is this economic pressure in a good way to make progress on alignment because long horizon agents require it. But I think it's an open question how much and what aspects of alignment have to be solved for this.\n",
            "Transcript: [1113.038 --> 1139.195]  Changing topics a little bit, I think this is a question that we all maybe think has been answered, which is, is there a set of data that can give you an advantage? If you go back in time just a handful of years before frontier models, before LLMs, the assumption, not just the assumption, the fact was that custom data mattered. If you wanted to build an AI startup or if you're an enterprise and you're trying to deploy AI, you had this massive advantage if you had a massive data set that was custom to your need.\n",
            "Transcript: [1139.195 --> 1155.614]  And that was actually the only way to get useful AIs, more than a handful of years ago, was by training models on your custom data set. And then very quickly, what we saw was LLMs got extremely powerful and extremely general. And actually, it was just better to use the general LLM than it was to train or even fine tune on your custom data, right?\n",
            "Transcript: [1155.732 --> 1183.323]  What I think is true is—maybe it's true, this is my open question—is there might be industries where that's not the case, where actually AI is maybe not great. An example might be like material science. Can an LLM do really good material science? Does a company that specializes in material science, that has decades of data, can they do better? And I think potentially yes, right? Like, LLMs are great at everything that they've found on the internet, but are they great at all this like tacit knowledge that's locked up in companies that actually hasn't bled out?\n",
            "Transcript: [1183.323 --> 1211.42]  I think about like TSMC or ASML, right? They make these multi-billion dollar bets and they do an incredibly good job of keeping all that tacit knowledge they build in-house. It doesn't leak out. Frontier LMs do not know how to build a cutting edge semiconductor fab. That's an important fact, actually. And I think if you're a startup thinking about where there's a defensible position, and I'll touch on this again in a few slides, I think that's like an important question to keep in mind.\n",
            "Transcript: [1211.487 --> 1239.905]  Okay, different tact here is you're probably all aware of like capacity issues. Everyone's trying to scale extremely quickly. I think there's demand from consumers, from your consumers potentially, from your startups to scale extremely fast, right? Maybe we want to scale like 100x over the next couple of years. That's faster than we can scale GPU production. So what are we going to do? And I think there's a lot of open questions around like, does fine tuning actually matter? I think a lot of people have abandoned fine tuning and said, actually, I'm just going to do better context management.\n",
            "Transcript: [1240.192 --> 1254.535]  Or is there like a role to play in like better routers between which model, small versus large, et cetera, right? So I think this is like a place where if you're interested in the technical details, you can have a competitive advantage, at least for the next year or two, because capacity is really going to matter, right?\n",
            "Transcript: [1255.497 --> 1270.668]  Often from a product perspective, I like to say, make it great, then make it scale. If you're already starting to work on the make it scale part, this is really important to you. And this can be some of the technical moat that you can build that gets you ahead of the competitors.\n",
            "Transcript: [1270.668 --> 1300.604]  Getting ahead of the competitors, though, is like it's a rat race. So at some point, the models will get better, the capacity problems will improve, and then that advantage might go away. So you need to have a better answer to this question of like, what's your moat? How are you going to stay ahead? What makes a durable advantage in a post-AGI world, right? In two years or three years, if I can just prompt, you know, Cloud 7 or GPT-7 to just replicate your startup, what's your advantage going to be, right?\n",
            "Transcript: [1300.604 --> 1323.571]  And if I'm a megacorp and I have more money than you and I can throw more tokens at that, are you really going to have a durable advantage or are you just going to get clobbered by megacorp? I think it's a real question that is extremely serious. Even before AGI, I like to work on hard problems. That's the moat for me. Everyone has a different type of moat. Some people are great at marketing or whatever, right? I like solving hard problems.\n",
            "Transcript: [1323.571 --> 1347.618]  But I think about what does it mean to be a hard problem? What's going to be hard in a post-AGI world? And I think a lot of things, actually, like TSMC, like ASML, those are hard problems that eventually will get easy with robotics, etc. But robotics are lagging behind. So I think there's these hard problems that will exist even in two years. And if you're willing to go after hard problems, if you have the guts to do that,\n",
            "Transcript: [1347.618 --> 1363.615]  Then you can have a massive competitive advantage, right? So what is that set of hard problems? Infrastructure and energy and manufacturing and chips. What else? Those are the things that are top of mind for me, but what are your answers to this question? What's still going to be hard and worth doing?\n",
            "Transcript: [1363.615 --> 1383.73]  I think about this question a lot too, like, is there an intelligence ceiling to what you need for various different tasks, right? Like, you know, if you look at ImageGen, for example, you know, just recently with Veo 3, with VideoGen, it's like, I can, I finally am getting fooled by videos and like my Instagram feed is starting to get full up of\n",
            "Transcript: [1383.73 --> 1407.592]  Like, Capybara's singing songs and shit. And I'm like, actually, this is great. I love these videos. That wasn't true three months ago, right? But can it get even better? Or is there, like, for a specific task, for a specific use case, is there, like, some max where it's like, yeah, that's, it's good enough. We've saturated, right? Like, that's the best you can get at writing a poem. That's the best you can get at writing a git diff for a PR. That's, you know,\n",
            "Transcript: [1407.592 --> 1431.436]  An important question, because if there is a ceiling, then the commoditization for that task is going to hit much sooner, right? You actually can't stay on the edge longer by moving to the next model, figuring out how to prompt the next model. The task saturates, right? So the commoditization pressure will be even more extreme. So I think it's important to think about this. And again, I think it's going to depend on the task and the vertical. Like some things, maybe there won't be a ceiling. Some things, maybe there will.\n",
            "Transcript: [1431.807 --> 1451.264]  This is changing tack a little bit here, but is there going to be a need for neutrality? Since the dawn of chatbots two or three years ago or whatever, people have been complaining about refusals as an example. Oh, I asked the model to do this and it refused. If we end up relying on these models, that's a massive\n",
            "Transcript: [1451.264 --> 1472.645]  Question for society, right? That there's going to be a handful of corporations that get to decide what is okay and not okay for an AI to do for you. And if we start relying on AI to do everything, then those companies become arbiters of what gets built, right? That's extremely important. So is there going to need to be a notion of neutrality? Like we have today with some forms of infrastructure, right? Electrical infrastructure is\n",
            "Transcript: [1472.645 --> 1495.19]  If GE owned all the electrical grid and they were like, you can only use our electrical grid if you promise to plug in our toaster oven to it, like no other toaster oven is allowed to work on our electrical grid, that would be a problem. And it's good that that didn't happen. Obviously, we fought and lost this battle for the web. What's going to happen with AI? Do we need AI neutrality? Token neutrality? I don't know what we call it.\n",
            "Transcript: [1495.578 --> 1518.68]  I'll wrap up my set of questions here. I have an infinite number of questions, and I hope that these just stimulated you to think a little bit as well about not just these questions, but your own questions, right? I want to just touch on one thing that's always inspired me about Silicon Valley, and it's this cringe thing that has always—you'll only see it in an ironic sense, but actually I think it's really great, which is\n",
            "Transcript: [1518.68 --> 1541.714]  In Silicon Valley, we always used to say, like, what's your startup doing? And someone's like, oh, I'm trying to change the world. Everyone always wanted to, like, think big and do the big thing. And, like, at the end of the day, it was a cat sharing app or whatever, right? But, like, the desire to change the world was there. I think startup founders really meant it when they said, I want to change the world. I think the thing that's concerning to me is, you know, I've been on the bleeding edge of AI for, like, ever and I've\n",
            "Transcript: [1541.714 --> 1567.027]  You know, I've been worried about AI risks forever. And, you know, five years ago when I talked to someone about AI, they'd be like, what the fuck are you talking about? You think these things are going to maybe kill us or whatever? Like, this is insane. Now, when I talk to people, you know, I sit them down and I explain everything that's happening. They already are aware, obviously, but I'm like, let me give you some more details. And suddenly they're like, oh, you can see the gears start turning. And just regular folks that I talk to in my life, they get it.\n",
            "Transcript: [1567.027 --> 1588.947]  And they're like viscerally aware that the things that we're about to go through are extremely important. They're like humanity-defining, society-defining. Like people get this, right? It's not a nuanced thing, right? We're for the first time ever bringing a second intelligence in the world that matches humanity and will eventually exceed\n",
            "Transcript: [1588.947 --> 1606.936]  Okay, how do we make money off of this?\n",
            "Transcript: [1607.847 --> 1635.708]  And I get extremely disappointed in that person. And I totally understand why they're asking this question, right? Like, you know, there's a lot of fear in the world. I think the fear of like all of us losing our jobs or not being able to build a startup that's going to be competitive anymore, or maybe the door to build a startup closes forever in the next few years. Who knows, right? Like there's a lot of fear. And I think AI is scary, right? So like people start thinking, well, I need to make money right now. Like if the economy is going to be fundamentally different in the next few years and I can't guarantee my place in it,\n",
            "Transcript: [1635.708 --> 1655.587]  Jordan Fisher is the co-founder & CEO of Standard AI and now leads an AI alignment research team at Anthropic. In his talk at AI Startup School on June 17th, 2025, he frames the future of startups through questions rather than answers—asking how founders should navigate a world where\n",
            "Transcript: [1656.498 --> 1675.55]  In that same moment, this is the last opportunity that we might have to make a difference, to change the world. If you're building a product, you need to use this moment. This might be the last product you build. This might be the last company you build. If you're inside of a company, the same applies. This might be the last chance that you have over the next couple of years.\n",
            "Transcript: [1675.55 --> 1700.423]  To make that impact, that could change the world, even in a small way. So if you have something you care about, now is the time to do it. I think about YC a lot and, you know, the slogan, first part is YC's slogan, build something people want, right? That's what we all want to do. And I think the truth is what people want often is something that's good for society. But you do need to think a little bit deeper, like,\n",
            "Transcript: [1700.609 --> 1724.234]  I think people do want things they can trust. They want agents they can trust. They want bots they can trust. They want to know that when they use a product, it's not just going to delight them for the next 20 seconds, that it's going to be good for their mental health for the next 20 years, or the mental health of their children or their neighbors. We want good things. We as consumers and users, we want good things, right? And when we say build something people want,\n",
            "Transcript: [1724.234 --> 1753.293]  Don't just think about what people will consume. Like, what does society need? I think if you build the right thing, a lot of people will want it. So I'll wrap up and I'll say, like, I know there's a lot, like, I get stressed out thinking about this sometimes, but I really think as founders or as people who are interested in being founders, like, we have this unique perspective. Like, we think about things in a way that most other people don't, right? Like, it's kind of the whole job of being a founder is finding an edge. That's the whole thing.\n",
            "Transcript: [1753.293 --> 1771.18]  And I think over the next couple of years, things are just going to move extremely fast and the rules are going to change every six months and you're going to have to think again, right? And if there's anyone that's positioned to stay at the bleeding edge of that, understand how those changes are impacting some of these questions and other questions that we're talking about.\n",
            "Transcript: [1771.18 --> 1795.565]  And then use the insights from the answers to those questions to drive positive change. If there's anyone that can do that, it's you all here. It's folks that care about thinking about these things and care about building things that people want. So I hope that that's what you all do. I hope you make money doing it too while you can. And yeah, thanks for listening. I think we got time for some questions.\n",
            "Transcript: [1802.163 --> 1825.771]  Thank you so much for the talk. I think that was probably one of the more grounded and down-to-earth talks I've heard, especially the last point about build something people want. I've always thought it's build something the world needs is perhaps a bit more important for a time like this. And so I guess my question for you coming up with all those questions on how to approach AGI and the insights that come along with it, what are a couple sources of information that\n",
            "Transcript: [1826.058 --> 1856.045]  What inspired you to do this, whether that's people, podcasts, books, and things like that? What have been most helpful for building your mental model?\n",
            "Transcript: [1856.045 --> 1880.345]  A limited energy budget to digest new ideas. Don't just maximize for people you agree with, maximize for diversity. I'm in reinforcement learning context all the time and there's this notion of diversity in RL, exploration versus exploitation. You want to make sure you're doing a lot of exploration in your information diet before you're doing the exploitation, which is like starting a company, for example.\n",
            "Transcript: [1880.699 --> 1910.315]  Thanks for the amazing talk, guys. Probably my favorite talk today. Something I think about quite a bit is, like, in a world where AGI is coming, say, in two or three years, traditionally, like, the questions I ask myself is, if I have a couple of startup ideas, should I work on something that I'm really passionate about and have experience with expertise in? Should I work on some place, an idea where the market's underserved or not as competitive? In a place where AGI is coming, should the most singular important question be, what idea is the most defensible against AGI?\n",
            "Transcript: [1910.416 --> 1932.877]  It's a great set of questions. I think even before all this AGI stuff, a lot of those were good questions. Personally, my opinion is that once you're six months into 100-hour work weeks, I don't care how passionate you are about an idea, you're going to fucking hate it. The only thing that's going to keep you going is your desire to have the impact, your commitment to the company, your commitment to your co-founders and your team.\n",
            "Transcript: [1932.877 --> 1958.628]  So does it really matter to be extremely passionate about the domain that you're going after? In my opinion, no. But I think other people feel differently. So that's a personal question. But I do think being impact-oriented is really important. And whether or not you're trying to have an impact or not, I think defensibility really is, in my opinion, one of the key questions, right? I mean, that's kind of half the talk, I guess, right? It's like, what's going to change? And is the thing you're building just going to be a rounding error over the next six months?\n",
            "Transcript: [1958.965 --> 1986.893]  I honestly think there's a lot of money to be made on a 6-18 month horizon. If all you're optimizing for is hockey stick curve, grow your ARR, flip the company, and make a quick buck, you might not need something long-term defensible. But if you want to build something that's going to stand the test of time and be part of this transition through the singularity and all the craziness of that, I would say think harder about the defensibility. That's probably the most important thing. Thank you so much. Good question.\n",
            "Transcript: [1987.248 --> 2008.662]  Some of those questions are really mind-bending. I've been thinking about them a lot as well. My question is, I know you've tried to keep it open, but what's your personal opinion on the value of money? Do you think as the cost of goods and services go down, money will become less valuable because everything's free, or it will become more valuable because we can do way more stuff?\n",
            "Transcript: [2008.848 --> 2033.738]  Yeah, it's a great question. Yeah. I mean, I think there's going to be policy decisions that impact this a lot, right? Like, do we need some form of UBI or like, you know, maybe like slightly more weird, like universal basic compute? Like if compute is the thing that powers everything, are we all entitled to some form of compute? These are like open policy questions that will start impacting that answer. And I think we should think seriously about them because they will also change the nature of our society and the sort of like\n",
            "Transcript: [2033.738 --> 2063.725]  Thanks, Jordan. Thanks for having me.\n",
            "Transcript: [2063.725 --> 2092.278]  But once AGI arrives, you don't need labored buy-in anymore, right? You don't need folks like me to approve of the thing that you're building or of your morals or whatever, right? Capital begets capital in that world, and that can easily spiral out of control, right? We already have a lot of concentration of wealth. That could really spiral out of control. So I think we're between a rock and a hard space, and I don't know what the answer is. But I think the policy decisions we make will have a huge impact on the question that you asked. Yeah, thank you. Yeah, good question.\n",
            "Transcript: [2092.582 --> 2109.912]  Hey Jordan, thanks so much for the time. I guess the question I wanted to ask is how do you think about alignment at the level of individual users? And I guess how important is that for trust, especially given that preferences evolve over time and you don't want to keep retraining models, especially also when you have a lot of users?\n",
            "Transcript: [2110.216 --> 2125.235]  Yeah, I mean, I think like everything—I have like a startup product lens on everything, right? And it's like startup mantra is that users don't know what they want, right? And I think that's true to a large extent, right? But I think users still have values and you want to discover those values and be—and honor them to some extent, right?\n",
            "Transcript: [2125.235 --> 2141.823]  So I think like something that's top of mind for me is, you know, you probably saw like the sycophantic behavior from the recent, you know, other other AI provider. And like, I think if you put in front of a user two responses and one of them is more sycophantic, one's like glazing, I guess is what the kids say nowadays, right?\n",
            "Transcript: [2141.823 --> 2159.39]  Then a lot of users will pick the sycophantic response, right? Like in that moment, they're like, yeah, of course, like, of course, the question I'm asking is a great fucking question. Thanks for recognizing it, right? But I think if you take a step back and you say, hey, hold on a second, here's two principles\n",
            "Transcript: [2159.39 --> 2188.162]  And you can choose the AI that follows this principle or this principle. And the first principle is that we're never going to blow smoke up your ass. We're only going to tell you if we like something or if an idea is good, if it really is. And the other principle is, actually, we're just going to like, you know, we're just going to glaze you all day. Like, that's what we're going to do. If you ask the user, which principle do they want? Almost everyone's going to say the first one, right? So I think doing what the user wants, you get to a different answer depending on what level of the engagement you're asking them.\n",
            "Transcript: [2188.162 --> 2215.111]  And I think that's really important because people can abuse that to their advantage by only asking the user the question in certain ways, right? And I think you need to really ask yourself, what's the right way to ask the user this question to get at the heart of what is going to be best for them? I don't know if that gets to you. I kind of went off on a diatribe. That makes sense. Thanks so much. Cool. Great question. Yeah. Thank you for the talk. I really enjoyed it. It seems to me like you value critical thinking a lot. So I was wondering what topics or what, like,\n",
            "Transcript: [2216.073 --> 2221.49]  What opinions does the general crowd in the tech field have that you disagree with?\n",
            "Transcript: [2221.726 --> 2247.173]  I guess my high-level general statement that I'll tell you, and I'm happy to talk more offline, I think despite the fact that we say our industry is this forward-looking industry and we're bold and we love taking risks and putting everything on the line and seeing what other people don't see, I think the truth is that there's an extreme amount of groupthink. The products that we build, what gets funded by VCs, etc.\n",
            "Transcript: [2247.173 --> 2275.777]  And I think even today, like, you talk to a bunch of VCs and they're like, oh, yeah, like, I'm ahead of the curve, like, I'm investing in AI. I'm like, no, like, you're two years behind already, right? Like, tell me about what you think needs to happen in two years, right? Like, are you asking these types of questions of, like, what do I need to be investing in today so that it's resilient in two years? And I almost never see a VC asking that question, right? But if I was a VC, which I'm not, that would be my investment thesis. As a founder, that's my investment thesis. Like, I don't know if that answers your question, but happy to go into it. Thank you. Yeah, cool. Great question. Yeah.\n",
            "Transcript: [2276.047 --> 2303.536]  You mentioned how trust is going to be a bigger issue in the future. So do you think that blockchain might be a part of the solution there? Thank you. I will just preface by saying that I am a huge blockchain doubter. I don't know. But nonetheless, the price keeps going up and I don't get any of that because I refuse to buy it. But I do think, you know, in this world where we need trust,\n",
            "Transcript: [2303.536 --> 2332.848]  Yeah, that's the right set of ideas. We need ideas like that in others. Some of the ideas I was touching on is AI-powered audits. Can two different AI companies audit each other? How do we get to places where we can build trust? If we do end up with some form of universal basic income or basic compute, basic tokens, does that need to be mediated by a blockchain so that it's not just at the behest of a central government? Yeah, I think those are reasonable questions that maybe I could get behind a blockchain on.\n",
            "Transcript: [2332.848 --> 2353.823]  Google recently released an ATA protocol to standardize how agents would talk to agents, and we talked a little bit about trust on agents. I'm curious how you would envision a world of agents talking to agents, and how that would affect applications, and will there be a GenTech premise scheme?\n",
            "Transcript: [2353.823 --> 2371.255]  Yeah, it's a great question. I could talk about this all day, and I'm almost out of time. So maybe I'll just like, I'll give you one example where it's like, not obvious why this was hard. But like, think about like a personal assistant agent that's just scheduling meetings for you, right? Seems trivial, right? Like, oh, yeah, I'm just gonna look at this person's calendar, and then I'm gonna like,\n",
            "Transcript: [2371.255 --> 2390.695]  In his talk at AI Startup School on June 17th, 2025, he frames the future of startups rather than answers—asking how founders should navigate a world where\n",
            "Transcript: [2390.695 --> 2407.199]  Whereas if you say, yeah, I'm happy to schedule a meeting between you and Joe, but it's two weeks out, three weeks out, right? There's a power dynamic that you're doing, right? And the truth is, all these game theoretic things matter. A good human assistant knows all these things and abides by them.\n",
            "Transcript: [2407.199 --> 2432.917]  But it's all implicit, right? It's not like, oh, there's this like concrete piece of information that the agent has access to and that's the important part of the security. It's like way more subtle and semantic. So I think it's just hard, but I think it's a great question. Yeah. Thank you. Cool. And I think out of time, unfortunately, but feel free to like shoot me a message online or I don't know how to get like my Twitter's Jordan Fisher, Jordan Ezra Fisher. Shoot me a message. Always happy to chat. And thanks for the great questions and hope you all enjoy the talk.\n",
            "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_fairseq_large_lv60k_asr_ls960.pth\" to /root/.cache/torch/hub/checkpoints/wav2vec2_fairseq_large_lv60k_asr_ls960.pth\n",
            "100% 1.18G/1.18G [00:12<00:00, 99.9MB/s]\n",
            "2025-10-14 10:23:15 - whisperx.transcribe - INFO - Performing alignment...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# input = file_name.replace(\":\", \"\\:\").replace(\"'\", \"'\")\n",
        "input = file_name\n",
        "\n",
        "language_param = \"\"\n",
        "if language != \"auto\":\n",
        "    language_param = f\"--language {language}\"\n",
        "\n",
        "diarize_param = \"\"\n",
        "if assign_speaker_lable:\n",
        "    diarize_param = \"--diarize --hf_token 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\"\n",
        "\n",
        "align_whisper_param = \"\"\n",
        "if align_whisper_output:\n",
        "    align_whisper_param = \"--align_model WAV2VEC2_ASR_LARGE_LV60K_960H\"\n",
        "\n",
        "prompt_param = \"\"\n",
        "if prompt != \"\":\n",
        "    prompt_param = f'--initial_prompt \"{prompt}\"'\n",
        "\n",
        "run = f'whisperx \\'./{input}\\' --model {model_size}{language_param} --output_dir . {prompt_param} {align_whisper_param} {diarize_param}'\n",
        "\n",
        "print(run)\n",
        "\n",
        "!{run}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDuzo5iBSIo7"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "base_filename = os.path.splitext(file_name)[0]\n",
        "base_filename = base_filename.replace(\":\", \"\\:\")\n",
        "srt_filename =f\"{base_filename}.srt\"\n",
        "json_filename = f\"{base_filename}.json\"\n",
        "print(srt_filename)\n",
        "print(json_filename)\n",
        "files.download(srt_filename)\n",
        "files.download(json_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjnDpcfUFs--"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}